{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Methods for solving Laplace Equation\n",
    "\n",
    "In this notebook, we will learn how to solve the Laplace's equation with iteractive methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, njit, prange, set_num_threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesh Generation\n",
    "\n",
    "Before we start solving the Laplace's euqtion, we need to generate the mesh first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mesh(nx, ny, buff=1, \n",
    "                  xmin=0.0, xmax=1.0, ymin=0.0, ymax=1.0):\n",
    "    \"\"\"\n",
    "    Generate 2D mesh grids for solving Laplace equation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nx : int\n",
    "        Number of grid points in x direction.\n",
    "    ny : int\n",
    "        Number of grid points in y direction.\n",
    "    buff : int\n",
    "        Number of ghost cells around the domain.\n",
    "    xmin : float\n",
    "        Minimum value of x.\n",
    "    xmax : float\n",
    "        Maximum value of x.\n",
    "    ymin : float\n",
    "        Minimum value of y.\n",
    "    ymax : float\n",
    "        Maximum value of y.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : 2D numpy array\n",
    "        Mesh grid for x.\n",
    "    y : 2D numpy array\n",
    "        Mesh grid for y.\n",
    "\n",
    "    dx : float\n",
    "        Grid spacing in x.\n",
    "    dy : float\n",
    "        Grid spacing in y.\n",
    "        \n",
    "    \"\"\"\n",
    "    #TODO:\n",
    "    np.zeros((nx+2*buff,ny+2*buff))\n",
    "    x=np.linspace(xmin,xmax,nx+2*buff)\n",
    "    y=np.linspace(ymin,ymax,ny+2*buff)\n",
    "    dx=x[1]-x[0]\n",
    "    dy=y[1]-y[0]\n",
    "\n",
    "    return u, x, y, dx, dy\n",
    "   \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobi method\n",
    "\n",
    "Recall the finite difference equations for the Laplace's equation, we have\n",
    "\n",
    "$$\n",
    "u_{i,j}^{k+1} = \\frac{1}{4}(u_{i-1,j}^k + u_{i,j-1}^k + u_{i+1,j}^k + u_{i,j+1}^k)\n",
    "$$\n",
    "\n",
    "\n",
    "### Exercise: Jacobi method\n",
    "\n",
    "* A unit square with $N \\times N$ grids\n",
    "* Need one layer of ghost cells for boundary conditions\n",
    "* Boundary Conditions: 1 on the top BC; others are 0 \n",
    "* Once we have the mesh, implment the jacobi method by\n",
    "1. Write a function called `jacobi` to implment one Jacobi iteration\n",
    "2. Write a function called `update_bc` to update the boundary conditions.\n",
    "3. Write a function called `relax` to iterate the matrix unitl the error matches a tolerance (loop through $k$). \n",
    "* Error can be defined by $\\sqrt{\\sum (u_{\\rm ij}- u_{\\rm old,ij})^2}/N_{\\rm cells}$\n",
    "* The `jacobi` function need to be acclerated by `numba`\n",
    "* For debugging, you could start from a bigger tolerance first. \n",
    "* The your `relax` function with $32\\times 32$, $64 \\times 64$, and $128 \\times 128$ grids.\n",
    "* Plot errors vs. #iterations, to see how it converges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "@njit(parallel=True)\n",
    "def jacobi(u,uold,nx,ny):\n",
    "   for j in prange(1,ny-1):\n",
    "      for i in prange(1,nx-1):\n",
    "         u[i,j](uold[i-1,j]+uold[i,j-1]+uold[i+1,j]+uold[i,j+1])/4\n",
    "return u\n",
    "\n",
    "def update_bc(u)\n",
    " u[0,:]=0\n",
    " u[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gauss-Seidel Method\n",
    "\n",
    "* Gauss-Seidel meothd remedies this by using each new component of solution as soon as it has been computed. \n",
    "\n",
    "$$\n",
    "{\\rm Gauss-Seidel: } u_{i,j}^{k+1} = \\frac{1}{4}(u_{i-1,j}^{k+1} + u_{i,j-1}^{k+1} + u_{i+1,j}^k + u_{i,j+1}^k)\n",
    "$$\n",
    "\n",
    "\n",
    "* The Gauss-Seidel method averages solution values at four surrounding grid points, but always use new component values as soon as they become available, rather than waiting until current iteration has been completed.\n",
    "* No need a copy of `u`.\n",
    "* Gauss-Seidel method does not always converge, but it is guaranteed to converge under conditions that are often satisfied in practice.\n",
    "* Although Gauss-Seidel converges more rapidly than the Jacobi method, it is often still too slow to be practical.\n",
    "\n",
    "### Exercise: Gauss-Seidel Method\n",
    "\n",
    "* Modify your `relax` function to support the Gauss-Seidel Method\n",
    "* write a function `gauss_seidel()` to implement one Gauss-Seidel iteration.\n",
    "* The `gauss_seidel()` function need to be accelerated by numba.\n",
    "* Compare the number of iterations (and the computing time) between Jacobi method and Gauss-Seidel method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Successive Over-Relaxation (SOR)\n",
    "\n",
    "* Convergence rate of Gauss-Seidel can be accelerated by successive over-relaxation (SOR) method. \n",
    "* Starting with $x^k$ first, compute next iterate that would be given by Gauss-Seidel $x_{GS}^{k+1}$, then instead take next iterate to be\n",
    "\n",
    "$$\n",
    "x^{k+1} = x^k + \\omega(x_{GS}^{k+1} - x^k) = (1-\\omega)x^k + \\omega x_{GS}^{k+1}\n",
    "$$\n",
    "\n",
    "*  Which is weighted average of current iterate and next GS iterate\n",
    "* $w$ is a fixed relaxation parameter chosen to accelerate convergence\n",
    "* $w > 1$ gives over-relaxation\n",
    "* $w<1$ gives under-relaxation\n",
    "* $w=1$ gibes Gauss-Seidel meothd\n",
    "* Method diverges unless $0 < w < 2$, but choosing optimal $w$ is difficult in general. \n",
    "\n",
    "### Exercise: Successive Over-Relaxation\n",
    "\n",
    "* Consider $64 \\times 64$ first for developing the solver\n",
    "* Write a function called `successive_over_relax()` to implement one SOR iteration.\n",
    "* Modify the `relax()` to support the SOR method.\n",
    "* The `successive_over_relax()` function need to be accelerated by `numba`.\n",
    "* Compare the number of iterations (and the computing time) with all other methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comphys-112",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
